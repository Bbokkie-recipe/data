{
    "copyright_text": "CC BY",
    "description": "You\u2019ve heard about ChatGPT\u2019s conversational ability and how DALL-E can\ncreate images from a simple phrase. Now, you want to get your hands\ndirty training some state of the art (SOTA) deep learning models. We\nwill use Jupyter notebooks to fine-tune an NLP model based on BERT to do\nsentiment analysis.\n\nIn this hands-on tutorial, we will learn about using HuggingFace models\nfrom pre-trained open-source checkpoints and adapting these models to\nour own specific tasks. We will see that using SOTA NLP and computer\nvision models has been made easier with a combination of HuggingFace and\nPyTorch.\n\nAt the end of this session, you will know how to fine-tune a large\npublic pre-trained model to a particular task and have more confidence\nnavigating the deep learning open source landscape.\n",
    "language": "eng",
    "recorded": "2023-04-19",
    "related_urls": [
        {
            "label": "Conference Website",
            "url": "https://us.pycon.org/2023/"
        },
        {
            "label": "Presentation Webpage",
            "url": "https://us.pycon.org/2023/schedule/presentation/103/"
        }
    ],
    "speakers": [
        "Juhi Chandalia",
        "Dana Engebretson"
    ],
    "thumbnail_url": "https://i.ytimg.com/vi/-249nmQfv1U/maxresdefault.jpg",
    "title": "Intro to Hugging Face: Fine-tuning BERT for NLP tasks",
    "videos": [
        {
            "type": "youtube",
            "url": "https://www.youtube.com/watch?v=-249nmQfv1U"
        }
    ]
}