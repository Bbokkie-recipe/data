{
    "copyright_text": "CC BY",
    "description": "With the advent of large pre-trained language models like GPT, BERT,\netc., and their usage in almost all natural language understanding and\ngeneration applications, it is important that we evaluate the fairness\nand mitigate biases of these models. Since these models are fed with\nhuman-generated data (mostly from the web), they are exposed to human\nbiases. Hence, they carry forward and also amplify these biases in their\nresults. In this talk, we will discuss the motivation for fairness and\nbias research in NLP and discuss different approaches used to detect and\nmitigate biases. We will also explore some available tools to include in\nyour models to ensure fairness.\n",
    "language": "eng",
    "recorded": "2023-04-22",
    "related_urls": [
        {
            "label": "Conference Website",
            "url": "https://us.pycon.org/2023/"
        },
        {
            "label": "Presentation Webpage",
            "url": "https://us.pycon.org/2023/schedule/presentation/56/"
        }
    ],
    "speakers": [
        "Angana Borah"
    ],
    "thumbnail_url": "https://i.ytimg.com/vi/f0bEx1yT72o/maxresdefault.jpg",
    "title": "Approaches to Fairness and Bias Mitigation in Natural Language Processing",
    "videos": [
        {
            "type": "youtube",
            "url": "https://www.youtube.com/watch?v=f0bEx1yT72o"
        }
    ]
}